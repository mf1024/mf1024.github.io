---
layout: post
title: "MixMatch: A Holistic Approach to Semi-Supervised Learning"
comments: true
---
> **notes on [https://arxiv.org/abs/1905.02249 ](https://arxiv.org/abs/1905.02249)*
## Key contributions and ideas of this paper:
- MixMatch combines current dominant SSL approaches to create a new algorithm
- mixUp is used as both regularizer and semi-supervised learning method
- MixMatch reaches state-of-the-art accuracy on SSL benchmarks by a large margin


This paper presents Semi-Supervised Learning algorithm that combines current dominant approaches in a new algorithm called MixMatch. MixMatch reaches state-of-the-art accuracy in semi-supervised learning benchmarks by a large margin:

### Accuracy on the CIFR-10 and SVHN datasets:
![mixmatch accuracy_cifr10_cvhn](/img/mixmatch/accuracy.png)



Many of the current Semi-Supervised Learning approaches add loss term on unlabeled data to leverage the unlabeled data and encourage the model to generalize better.

### The added loss terms fall into the following classes:
- **Consistency regularization**
- **Entropy minimization**
- **Traditional regularization loss term**

### Consistency regularization

A typical consistency regularization method is data augmentation.

Consistency regularization tries to reduce:

$$ \| \mathrm{p}_{\text { model }}(y | \text { Augment }(x) ; \theta)-\mathrm{p}_{\text { model }}(y | \text { Augment }(x) ; \theta) \|_{2}^{2} $$

which means that the model for different augmentations should predict the same class distribution.

### Entropy minimization

Low-entropy prediction means that its high in confidence. Entropy minimization means making the model do confident predictions. One way to enforce entropy minimization is to require the model to output low-entropy predictions on unlabeled samples. MixMatch does that with the sharpening function. 

### Traditional regularization

Traditional regularization loss terms are added to  make it harder for the model to memorize the training data and so make it generalize better to unseen data. 

MixMatch uses MixUp both as a regularizer and semi-supervised learning method. Mixed samples from mixUp are used on both labeled and unlabeled samples and their loss terms. 


## The MixMatch Loss terms

The loss of MixMatch is:

$$
\mathcal{L}=\mathcal{L}_{\mathcal{X}}+\lambda_{\mathcal{U}} \mathcal{L}_{\mathcal{U}}
$$

Where $$ \lambda_{\mathcal{U}} $$ is a hyperparameter and $$ \mathcal{L}_{\mathcal{U}}, \mathcal{L}_{\mathcal{X}} $$ are the Labeled  and Unlabeled loss terms:

$$
\mathcal{L}_{\mathcal{X}}=\frac{1}{\left|\mathcal{X}^{\prime}\right|} \sum_{x, p \in \mathcal{X}^{\prime}} \mathrm{H}\left(p, \mathrm{p}_{\text { model }}(y | x ; \theta)\right)
$$

$$
\mathcal{L}_{\mathcal{U}}=\frac{1}{L\left|\mathcal{U}^{\prime}\right|} \sum_{u, q \in \mathcal{U}^{\prime}}\|q-\operatorname{p}_{\text { model }}(y | u ; \theta)\|_{2}^{2}
$$

$$ \mathcal{X}^{\prime} $$ and $$ \mathcal{U}^{\prime} $$ are the mixed Labeled and Unlabeled sets generated by the MixMatch algorithm.

$$ \mathrm{H}(p, q) $$ is the cross-entropy loss.

$$ \left\| q-p \right\|_{2}^{2} $$  is the $$ L_{2} $$ loss.

$$ L $$ is the number of possible labels.


## The MixMatch algorithm

### The Hyperparameters
Labeled and Unlabeled sets are mixed with MixMatch algorithm:

$$
\mathcal{X}^{\prime}, \mathcal{U}^{\prime}=\operatorname{MixMatch}(\mathcal{X}, \mathcal{U}, T, K, \alpha)
$$

- $$ \mathcal{X}=\left(\left(x_{b}, p_{b}\right) ; b \in(1, \ldots, B)\right) $$ is labeled data split into batches $$ B $$ with the corresponding one-hot targets $$ p $$
- $$ \mathcal{U}=\left(u_{b} ; b \in(1, \ldots, B)\right) $$ is an equally sized set of unlabeled data
- $$ T $$ is the temperature for Sharpening 
- $$ K $$ is the number of augmentation cycles for unlabeled data
- $$ \alpha $$ is beta distribution parameter for MixUp


### The pseudocode

0: $$ \boldsymbol{\operatorname{MixMatch}(\mathcal{X}, \mathcal{U}, T, K, \alpha)} $$: <br/> 
*$$ \;\;\, { \text { Iterate through the batches:}} $$* <br/>
1: $$ \boldsymbol{\text { for } b=1 \text { to } B \text { do }} $$ <br/>
*$$ \qquad\;\;\; { \text { Create augmentation of the labeled batch:}} $$* <br/>
2: $$ \boldsymbol{\qquad \hat{x}_{b}=\text { Augment }\left(x_{b}\right)} $$ <br/>
*$$ \qquad\;\;\; { \text { Create K augmentations of the unlabeled batch:}} $$* <br/>
3: $$ \boldsymbol{\qquad \text { for } k=1 \text { to } K \text { do }} $$ <br/>
4: $$ \boldsymbol{\qquad \qquad \hat{u}_{b, k}=\text { Augment }\left(u_{b}\right)} $$ <br/>
5: $$ \boldsymbol{\qquad \text { end for }} $$ <br/>
*$$ \qquad\;\;\; { \text {Get the mean predictions(over the K augmentations) of the unlabeled batches:}} $$* <br/>
6: $$ \boldsymbol{\qquad \overline{q}_{b}=\frac{1}{K} \sum_{k} {p}_{\text { model }}\left(y | \hat{u}_{b, k} ; \theta\right)} $$ <br/>
*$$ \qquad\;\;\; { \text {Sharpen the distribution of the mean predictions:}} $$* <br/>
7: $$ \boldsymbol{\qquad q_{b}=\text { Sharpen }\left(\overline{q}_{b}, T\right)} $$ <br/>
8: $$ \boldsymbol{\text { end for }} $$ <br/>
*$$ \;\;\;\; { \text {Create a labeled sample list from augmented labeled samples and their targes:}} $$* <br/>
9: $$ \boldsymbol{\hat{\mathcal{X}}=\left(\left(\hat{x}_{b}, p_{b}\right) ; b \in(1, \ldots, B)\right)} $$ <br/>
*$$ \;\;\;\;\; { \text {Create an unlabeled sample list from K augmentations of unlabeled samples}} $$* <br/>
*$$ \;\;\;\;\; { \text {and the sharpened means of predictions as targets:}} $$* <br/>
10: $$ \boldsymbol{\hat{\mathcal{U}}=\left(\left(\hat{u}_{b, k}, q_{b}\right) ; b \in(1, \ldots, B), k \in(1, \ldots, K)\right) } $$ <br/>
*$$ \;\;\;\;\; { \text {Shuffle the labeled and unlabeled samples into one list:}} $$* <br/>
11: $$ \boldsymbol{\mathcal{W}=\text { Shuffle }(\operatorname{Concat}(\hat{\mathcal{X}}, \hat{\mathcal{U}})) } $$  <br/>
*$$ \;\;\;\;\; { \text {mix each sample from the labeled list with some samples from the concatenated list using the mixUp:}} $$* <br/>
12: $$ \boldsymbol{\mathcal{X}^{\prime}= (\operatorname{Mix} \operatorname{Up}(\hat{\mathcal{X}}_{i}, \mathcal{W}_{i}) ; i \in(1, \ldots,|\hat{\mathcal{X}}|) ) } $$ <br/>
*$$ \;\;\;\;\; { \text {mix each sample from the unlabeled list with the rest of the samples from the concatenated list:}} $$* <br/>
13: $$ \boldsymbol{\mathcal{U}^{\prime}=(\operatorname{Mix} \operatorname{Up}(\hat{\mathcal{U}}_{i}, \mathcal{W}_{i+|\hat{\mathcal{X}}|}) ; i \in(1, \ldots, | \hat{\mathcal{U}} |))} $$ <br/>
14: $$ \boldsymbol{\text { return } \mathcal{X}^{\prime}, \mathcal{U}^{\prime}} $$ <br/>


### The mixUp function

The idea of mixUp function is taken from [https://arxiv.org/abs/1710.09412 ](https://arxiv.org/abs/1710.09412), where it was used in supervised learning to encourage linear behavior of predictions between examples. 

mixUp is an operation that mixes two samples and their labels $$(x_1,y_1)$$, $$(x_2,y_2)$$ into one resulting sample $$(x^{\prime},y{\prime})$$ in the following way:

$$ 
\lambda \sim \operatorname{Beta}(\alpha, \alpha) 
$$ 

$$ 
\lambda^{\prime}=\max (\lambda, 1-\lambda) 
$$ 

$$ 
x^{\prime}=\lambda^{\prime} x_{1}+\left(1-\lambda^{\prime}\right) x_{2} 
$$

$$ 
p^{\prime}=\lambda^{\prime} p_{1}+\left(1-\lambda^{\prime}\right) p_{2} 
$$ 



### The Sharpen function

The sharpen function is often used to adjust categorical distribution temperature $$ T $$. As $$ T $$ approaches zero, the distribution approaches Dirac or "one-hot" distribution.


$$
\text { Sharpen }(p, T)_{i} :=p_{i}^{\frac{1}{T}} / \sum_{j=1}^{L} p_{j}^{\frac{1}{T}}
$$

In the MixMatch algorithm, the sharpen function is used(line 7 in the pseudocode) on the mean of predictions on K augmentations of an unlabeled sample. Sharpen function is used for Entropy Minimization, enforcing the model to make a confident prediction on the unlabeled samples.

![mixmatch sharpening](/img/mixmatch/sharpening.png)


